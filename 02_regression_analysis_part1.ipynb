{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Conducting and Evaluating Regression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tqdm\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from src import ana_utils as utils\n",
    "\n",
    "#np.set_printoptions(suppress=True)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import datasets that were preprocessed in Notebook 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "wb_data = pd.read_csv(\"data/wb_data.csv\", index_col=\"Country Name\")\n",
    "wb_data_short = pd.read_csv(\"data/wb_data_short.csv\", index_col=\"Country Name\")\n",
    "whr_data = pd.read_csv(\"data/whr_data.csv\", index_col=\"Country name\")\n",
    "\n",
    "# test: are the same countries present in each dataset?\n",
    "print(sorted(list(wb_data.index))==sorted(list(whr_data.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test set. We choose a 80/20 split, i.e. 120 countries in the training set and 30 countries in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 120) (30, 120) (120, 1) (30, 1)\n",
      "True True\n"
     ]
    }
   ],
   "source": [
    "test_size = 30\n",
    "train, test, train_gt, test_gt = utils.split_data(wb_data_short, whr_data, test_size)\n",
    "\n",
    "# verify set shapes\n",
    "print(train.shape, test.shape, train_gt.shape, test_gt.shape)\n",
    "\n",
    "#verify that data prder and groundtruth order and indices match \n",
    "print(list(train.index)==list(train_gt.index), list(test.index)==list(test_gt.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's see how linear least squares regression performs on wb_data and wb_data_short (redundant indicators removed). We choose 2000-fold validation after noticing quite some variance for lower n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss (full set of indicators): 14.448268503467897\n",
      "The average size of the first ten coefficients ((full set of indicators)): [ 0.0153 -0.0082 -0.     -0.     -0.0029  0.0636 -0.0095  0.0731  0.0774\n",
      " -0.0862] \n",
      "\n",
      "Mean loss (reduced set of indicators): 14.507637341777448\n",
      "The average size of the first ten coefficients (reduced set of indicators): [ 0.015  -0.0075 -0.     -0.     -0.0029  0.0733  0.0033  0.07    0.0804\n",
      " -0.0808]\n"
     ]
    }
   ],
   "source": [
    "least_squares = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "# For the full dense indicator data\n",
    "loss_list, mean_loss, coef_list, avg_coefs = utils.n_fold_ceval(reg_model=least_squares, n=2000, data=wb_data, gt=whr_data, test_size=test_size, scaling=\"no_scaling\")\n",
    "print(\"Mean loss (full set of indicators):\", mean_loss)\n",
    "print(\"The average size of the first ten coefficients ((full set of indicators)):\", avg_coefs[:10], \"\\n\")\n",
    "\n",
    "loss_list, mean_loss, coef_list, avg_coefs = utils.n_fold_ceval(reg_model=least_squares, n=2000, data=wb_data_short, gt=whr_data, test_size=test_size, scaling=\"no_scaling\")\n",
    "print(\"Mean loss (reduced set of indicators):\", mean_loss)\n",
    "print(\"The average size of the first ten coefficients (reduced set of indicators):\", avg_coefs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While linear regression performs better after manually removing redundancies, both of the results are still quite poor. \n",
    "We suspect multicolinearity to be a main reason for bad performance. \n",
    "\n",
    "But before starting to deal with multicolinearity, we want to normalize/standardize the data. This is because, in the end, we aim to compare coefficients. Hence, performing analysis also on the normalized/standardized data along the way is necessary to prevent us from developing a model that works only on non-normalized/non-standardized data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization \n",
    "\n",
    "Here, we normalize each row, using the L2 norm. That is, for each country the indicator values are scaled such that the sum of all squared indicator values is one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss (full set of indicators): 31.28188481600015\n",
      "The average size of the first ten coefficients ((full set of indicators)): [ 28.7335 -15.5441  -0.1055  -0.523   -0.7349  60.7378  21.7037  67.2968\n",
      "   0.1485 -20.7325] \n",
      "\n",
      "Mean loss (reduced set of indicators): 4.944546921287039\n",
      "The average size of the first ten coefficients (reduced set of indicators): [ 21.4104  -7.3186   1.0311  -0.4875  -3.9778 101.6384 116.0385  23.1098\n",
      "  -5.2663   1.4116]\n"
     ]
    }
   ],
   "source": [
    "# For the full dense indicator data\n",
    "loss_list, mean_loss, coef_list, avg_coefs = utils.n_fold_ceval(reg_model=least_squares, n=2000, data=wb_data, gt=whr_data, test_size=test_size, scaling=\"normalize\")\n",
    "print(\"Mean loss (full set of indicators):\", mean_loss)\n",
    "print(\"The average size of the first ten coefficients ((full set of indicators)):\", avg_coefs[:10], \"\\n\")\n",
    "\n",
    "loss_list, mean_loss, coef_list, avg_coefs = utils.n_fold_ceval(reg_model=least_squares, n=2000, data=wb_data_short, gt=whr_data, test_size=test_size, scaling=\"normalize\")\n",
    "print(\"Mean loss (reduced set of indicators):\", mean_loss)\n",
    "print(\"The average size of the first ten coefficients (reduced set of indicators):\", avg_coefs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "Each value x is scaled with the formula $\\frac{x-\\mu}{\\sigma}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss (full set of indicators): 25.08420041301712\n",
      "The average size of the first ten coefficients ((full set of indicatoreg_model=least_squaresrs)): [ 0.6734 -0.1709  0.0026 -0.0407 -0.049   2.535   1.4409  1.1724  0.2123\n",
      " -0.9681] \n",
      "\n",
      "Mean loss (reduced set of indicators): 24.521177289572915\n",
      "The average size of the first ten coefficients (reduced set of indicators): [ 0.64   -0.1534  0.0173 -0.0243 -0.0197  2.4557  1.2371  1.2098  0.1055\n",
      " -1.0694]\n"
     ]
    }
   ],
   "source": [
    "# For the full dense indicator data\n",
    "loss_list, mean_loss, coef_list, avg_coefs = utils.n_fold_ceval(reg_model=least_squares, n=2000, data=wb_data, gt=whr_data, test_size=test_size, scaling=\"standardize\")\n",
    "print(\"Mean loss (full set of indicators):\", mean_loss)\n",
    "print(\"The average size of the first ten coefficients ((full set of indicatoreg_model=least_squaresrs)):\", avg_coefs[:10], \"\\n\")\n",
    "\n",
    "loss_list, mean_loss, coef_list, avg_coefs = utils.n_fold_ceval(reg_model=least_squares, n=2000, data=wb_data_short, gt=whr_data, test_size=test_size, scaling=\"standardize\")\n",
    "print(\"Mean loss (reduced set of indicators):\", mean_loss)\n",
    "print(\"The average size of the first ten coefficients (reduced set of indicators):\", avg_coefs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion for standardization/normalization\n",
    "Both methods seem to worsen mean loss for the regression on the full dataset. However, they don't change the mean loss significantly for the dataset containing the manually reduced set of indicators.  \n",
    "Therefore, we will from now on normalize the data in order to make the resulting regression coefficients more interpretable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
